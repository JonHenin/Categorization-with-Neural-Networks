{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonat\\Documents\\Masters\\DSC 550 - Data Mining\\Python\n"
     ]
    }
   ],
   "source": [
    "# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\n",
    "import os\n",
    "try:\n",
    "\tos.chdir(os.path.join(os.getcwd(), 'Python'))\n",
    "\tprint(os.getcwd())\n",
    "except:\n",
    "\tpass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "import keras\n",
    "#import keras_metrics as km\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from pandas import Series, DataFrame\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, cross_val_score, train_test_split, StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D \n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path('../Python/data/reddit/')\n",
    "path_categories = base_path / 'categorized-comments.jsonl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = pd.read_json(path_categories, lines=True, orient='columns') # Multiclass dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_cat.sample(frac=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cat'].unique()\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df['cat'])\n",
    "encoded_Y = encoder.transform(df['cat'])\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_preprocessor(text):\n",
    "    text = re.sub(\"\\S*\\d\\S*\", \"\", text).strip() # Strip out any numbers\n",
    "    text = text.translate(str.maketrans('','', string.punctuation)) # Strip out punctuation\n",
    "    return text.lower() # Return lowercase values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vector model of categories dataset\n",
    "cv_cat = TfidfVectorizer(preprocessor=my_preprocessor,\n",
    "                    stop_words='english')\n",
    "vectors_cv_cat = cv_cat.fit_transform(df['txt'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8332"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_cv_cat.todense().shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['MLPClassifier']\n",
    "columnnames = ['Model']\n",
    "\n",
    "# Set scoring tests to run\n",
    "scoring = {'Accuracy': 'accuracy',\n",
    "               'Precision': 'precision_macro',\n",
    "               'Recall': 'recall_macro',\n",
    "               'F1': 'f1_macro'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame which uses the models and datasets variables as initial values\n",
    "df_results = pd.DataFrame(columns=columnnames)\n",
    "df_results['Model'] = models\n",
    "\n",
    "# Create empty DataFrame columns for test results\n",
    "for k, v in scoring.items():\n",
    "    df_results[k] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURES = vectors_cv_cat.todense().shape[1]\n",
    "N_CLASSES = 4\n",
    "\n",
    "def insert_results(results, modelname, df):\n",
    "    \"\"\"Insert results of validation tests into a dataframe\n",
    "\n",
    "    Args:\n",
    "        results (dict): Dictionary results from validate_results()\n",
    "        modelname (str): Name of the model we are testing\n",
    "        df (dataframe): Dataframe we are inserting results into.\n",
    "    \"\"\"\n",
    "    for column, test in scoring.items():\n",
    "        df.loc[(df['Model'] == modelname),column] = np.mean(results['test_' + test])\n",
    "\n",
    "def validate_results(model, features, target, scoring, cv=2):\n",
    "    \"\"\"Run validation tests for a given model and provide scoring results.\n",
    "\n",
    "    Args:\n",
    "        model (model): Model function, e.g., MultinomialNB()\n",
    "        features (matrix): Matrix of features\n",
    "        target (array): array of target values\n",
    "        scoring (dict): Dictionary of scoring methods to return.\n",
    "        cv (int, optional): Number of cross validation folds. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of scoring results.\n",
    "    \"\"\"\n",
    "    cv_results = cross_validate(model,\n",
    "                features,\n",
    "                target,\n",
    "                cv=cv,\n",
    "                scoring=list(scoring.values()),\n",
    "                n_jobs=-1,\n",
    "                return_train_score=False)\n",
    "    return cv_results\n",
    "\n",
    "# create model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(500, activation='relu', input_shape=(N_FEATURES,)))\n",
    "    model.add(Dense(150, activation='relu'))\n",
    "    model.add(Dense(N_CLASSES, activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "    #model.fit(X_train, y_train, epochs=150, batch_size=10, verbose=0)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_results = validate_results(MLPClassifier(hidden_layer_sizes=[500, 150], verbose=True),\n",
    "                                vectors_cv_cat,\n",
    "                                df['cat'],\n",
    "                                scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_results(MLP_results, 'MLPClassifier', df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.528757</td>\n",
       "      <td>0.486375</td>\n",
       "      <td>0.385112</td>\n",
       "      <td>0.386993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Accuracy Precision    Recall        F1\n",
       "0  MLPClassifier  0.528757  0.486375  0.385112  0.386993"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.33585712\n",
      "Iteration 2, loss = 1.18810724\n",
      "Iteration 3, loss = 1.05777524\n",
      "Iteration 4, loss = 0.88433685\n",
      "Iteration 5, loss = 0.67159739\n",
      "Iteration 6, loss = 0.46861778\n",
      "Iteration 7, loss = 0.32154177\n",
      "Iteration 8, loss = 0.22990916\n",
      "Iteration 9, loss = 0.17218759\n",
      "Iteration 10, loss = 0.13976977\n",
      "Iteration 11, loss = 0.12134324\n",
      "Iteration 12, loss = 0.11354554\n",
      "Iteration 13, loss = 0.10886636\n",
      "Iteration 14, loss = 0.10728182\n",
      "Iteration 15, loss = 0.10575724\n",
      "Iteration 16, loss = 0.10606593\n",
      "Iteration 17, loss = 0.10614762\n",
      "Iteration 18, loss = 0.10455946\n",
      "Iteration 19, loss = 0.10317087\n",
      "Iteration 20, loss = 0.10319957\n",
      "Iteration 21, loss = 0.10268554\n",
      "Iteration 22, loss = 0.10367511\n",
      "Iteration 23, loss = 0.10295088\n",
      "Iteration 24, loss = 0.10301576\n",
      "Iteration 25, loss = 0.10198731\n",
      "Iteration 26, loss = 0.10350602\n",
      "Iteration 27, loss = 0.10303513\n",
      "Iteration 28, loss = 0.10575203\n",
      "Iteration 29, loss = 0.10516754\n",
      "Iteration 30, loss = 0.10177056\n",
      "Iteration 31, loss = 0.10302135\n",
      "Iteration 32, loss = 0.10237974\n",
      "Iteration 33, loss = 0.10323765\n",
      "Iteration 34, loss = 0.10311555\n",
      "Iteration 35, loss = 0.10227349\n",
      "Iteration 36, loss = 0.10287395\n",
      "Iteration 37, loss = 0.10161480\n",
      "Iteration 38, loss = 0.10362822\n",
      "Iteration 39, loss = 0.10328037\n",
      "Iteration 40, loss = 0.10048534\n",
      "Iteration 41, loss = 0.10297032\n",
      "Iteration 42, loss = 0.10215868\n",
      "Iteration 43, loss = 0.10144068\n",
      "Iteration 44, loss = 0.10134544\n",
      "Iteration 45, loss = 0.10285610\n",
      "Iteration 46, loss = 0.10142437\n",
      "Iteration 47, loss = 0.10158242\n",
      "Iteration 48, loss = 0.10237468\n",
      "Iteration 49, loss = 0.10222916\n",
      "Iteration 50, loss = 0.10237243\n",
      "Iteration 51, loss = 0.10116248\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33342269\n",
      "Iteration 2, loss = 1.20007621\n",
      "Iteration 3, loss = 1.06699365\n",
      "Iteration 4, loss = 0.90571730\n",
      "Iteration 5, loss = 0.69846872\n",
      "Iteration 6, loss = 0.49267753\n",
      "Iteration 7, loss = 0.34061307\n",
      "Iteration 8, loss = 0.24466722\n",
      "Iteration 9, loss = 0.18616183\n",
      "Iteration 10, loss = 0.15072582\n",
      "Iteration 11, loss = 0.13201399\n",
      "Iteration 12, loss = 0.12324398\n",
      "Iteration 13, loss = 0.11882004\n",
      "Iteration 14, loss = 0.11855130\n",
      "Iteration 15, loss = 0.11524748\n",
      "Iteration 16, loss = 0.11478710\n",
      "Iteration 17, loss = 0.11536980\n",
      "Iteration 18, loss = 0.11517945\n",
      "Iteration 19, loss = 0.11327585\n",
      "Iteration 20, loss = 0.11314270\n",
      "Iteration 21, loss = 0.11249731\n",
      "Iteration 22, loss = 0.11244360\n",
      "Iteration 23, loss = 0.11365798\n",
      "Iteration 24, loss = 0.11211298\n",
      "Iteration 25, loss = 0.11259217\n",
      "Iteration 26, loss = 0.11196015\n",
      "Iteration 27, loss = 0.11223089\n",
      "Iteration 28, loss = 0.11462833\n",
      "Iteration 29, loss = 0.11361871\n",
      "Iteration 30, loss = 0.11189814\n",
      "Iteration 31, loss = 0.11417278\n",
      "Iteration 32, loss = 0.11259182\n",
      "Iteration 33, loss = 0.11185964\n",
      "Iteration 34, loss = 0.11280463\n",
      "Iteration 35, loss = 0.11347520\n",
      "Iteration 36, loss = 0.11273633\n",
      "Iteration 37, loss = 0.11196347\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.32036064\n",
      "Iteration 2, loss = 1.18247042\n",
      "Iteration 3, loss = 1.04321908\n",
      "Iteration 4, loss = 0.86690550\n",
      "Iteration 5, loss = 0.65587081\n",
      "Iteration 6, loss = 0.45814931\n",
      "Iteration 7, loss = 0.32131699\n",
      "Iteration 8, loss = 0.23602302\n",
      "Iteration 9, loss = 0.18435710\n",
      "Iteration 10, loss = 0.15258574\n",
      "Iteration 11, loss = 0.13480127\n",
      "Iteration 12, loss = 0.12595106\n",
      "Iteration 13, loss = 0.12102676\n",
      "Iteration 14, loss = 0.11777243\n",
      "Iteration 15, loss = 0.11604118\n",
      "Iteration 16, loss = 0.11586664\n",
      "Iteration 17, loss = 0.11487183\n",
      "Iteration 18, loss = 0.11375120\n",
      "Iteration 19, loss = 0.11500841\n",
      "Iteration 20, loss = 0.11486162\n",
      "Iteration 21, loss = 0.11420634\n",
      "Iteration 22, loss = 0.11418184\n",
      "Iteration 23, loss = 0.11613384\n",
      "Iteration 24, loss = 0.11288759\n",
      "Iteration 25, loss = 0.11407573\n",
      "Iteration 26, loss = 0.11461407\n",
      "Iteration 27, loss = 0.11221633\n",
      "Iteration 28, loss = 0.11248492\n",
      "Iteration 29, loss = 0.11333382\n",
      "Iteration 30, loss = 0.11289090\n",
      "Iteration 31, loss = 0.11387678\n",
      "Iteration 32, loss = 0.11199708\n",
      "Iteration 33, loss = 0.11163013\n",
      "Iteration 34, loss = 0.11219152\n",
      "Iteration 35, loss = 0.11509956\n",
      "Iteration 36, loss = 0.11435131\n",
      "Iteration 37, loss = 0.11223467\n",
      "Iteration 38, loss = 0.11331165\n",
      "Iteration 39, loss = 0.11354123\n",
      "Iteration 40, loss = 0.11211374\n",
      "Iteration 41, loss = 0.11202353\n",
      "Iteration 42, loss = 0.11173987\n",
      "Iteration 43, loss = 0.11232896\n",
      "Iteration 44, loss = 0.11216266\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "y_pred = cross_val_predict(MLPClassifier(hidden_layer_sizes=[500, 150], verbose=True), \n",
    "                           vectors_cv_cat, \n",
    "                           df['cat'], \n",
    "                           cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(df['cat'], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[143,   5,  82, 158],\n",
       "       [ 33,   8,  31,  80],\n",
       "       [ 29,   6, 455, 273],\n",
       "       [ 59,   9, 293, 683]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=150, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jonat\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\jonat\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# evaluate using 3-fold cross validation\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=7)\n",
    "results = cross_val_score(model, vectors_cv_cat, df['cat'], cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5215361132383879"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(model, \n",
    "                           vectors_cv_cat, \n",
    "                           df['cat'], \n",
    "                           cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(df['cat'], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[124,  16,  94, 154],\n",
       "       [ 28,  14,  34,  76],\n",
       "       [ 33,  10, 449, 271],\n",
       "       [ 62,  15, 304, 663]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "                  news       0.50      0.32      0.39       388\n",
      "science_and_technology       0.25      0.09      0.14       152\n",
      "                sports       0.51      0.59      0.55       763\n",
      "           video_games       0.57      0.64      0.60      1044\n",
      "\n",
      "             micro avg       0.53      0.53      0.53      2347\n",
      "             macro avg       0.46      0.41      0.42      2347\n",
      "          weighted avg       0.52      0.53      0.52      2347\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(df['cat'], y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set that the color channel value will be first \n",
    "K.set_image_data_format(\"channels_first\")\n",
    "\n",
    "# Set seed \n",
    "np.random.seed(0)\n",
    "\n",
    "# Set image information \n",
    "channels = 1 \n",
    "height = 28 \n",
    "width = 28\n",
    "\n",
    "# Load data and target from MNIST data \n",
    "(data_train, target_train), (data_test, target_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape training image data into features \n",
    "data_train = data_train.reshape(data_train.shape[0], channels, height, width)\n",
    "\n",
    "# Reshape test image data into features \n",
    "data_test = data_test.reshape(data_test.shape[0], channels, height, width)\n",
    "\n",
    "# Rescale pixel intensity to between 0 and 1 \n",
    "features_train = data_train / 255 \n",
    "features_test = data_test / 255\n",
    "\n",
    "# One-hot encode target \n",
    "target_train = np_utils.to_categorical(target_train) \n",
    "target_test = np_utils.to_categorical(target_test) \n",
    "number_of_classes = target_test.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jonat\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Start neural network \n",
    "network = Sequential()\n",
    "\n",
    "# Add convolutional layer with 64 filters, a 5x5 window, and ReLU activation function \n",
    "network.add(Conv2D(filters=64,                   \n",
    "                   kernel_size=(5, 5),                   \n",
    "                   input_shape=(channels, width, height),                   \n",
    "                   activation='relu'))\n",
    "\n",
    "# Add max pooling layer with a 2x2 window \n",
    "network.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add dropout layer \n",
    "network.add(Dropout(0.5))\n",
    "\n",
    "# Add layer to flatten input \n",
    "network.add(Flatten())\n",
    "\n",
    "# # Add fully connected layer of 128 units with a ReLU activation function \n",
    "network.add(Dense(128, activation=\"relu\"))\n",
    "\n",
    "# Add dropout layer \n",
    "network.add(Dropout(0.5))\n",
    "\n",
    "# Add fully connected layer with a softmax activation function \n",
    "network.add(Dense(number_of_classes, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x266396251d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile neural network \n",
    "network.compile(loss=\"categorical_crossentropy\", # Cross-entropy                \n",
    "                optimizer=\"rmsprop\", # Root Mean Square Propagation                \n",
    "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "\n",
    "# Train neural network \n",
    "network.fit(features_train, # Features            \n",
    "            target_train, # Target            \n",
    "            epochs=2, # Number of epochs            \n",
    "            verbose=0, # Don't print description after each epoch            \n",
    "            batch_size=1000, # Number of observations per batch            \n",
    "            validation_data=(features_test, target_test)) # Data for evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
